{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Administrative</th>\n",
       "      <th>Administrative_Duration</th>\n",
       "      <th>Informational</th>\n",
       "      <th>Informational_Duration</th>\n",
       "      <th>ProductRelated</th>\n",
       "      <th>ProductRelated_Duration</th>\n",
       "      <th>BounceRates</th>\n",
       "      <th>ExitRates</th>\n",
       "      <th>PageValues</th>\n",
       "      <th>SpecialDay</th>\n",
       "      <th>Month</th>\n",
       "      <th>OperatingSystems</th>\n",
       "      <th>Browser</th>\n",
       "      <th>Region</th>\n",
       "      <th>TrafficType</th>\n",
       "      <th>VisitorType</th>\n",
       "      <th>Weekend</th>\n",
       "      <th>Revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Feb</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Feb</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Feb</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Feb</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>627.500000</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Feb</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Administrative  Administrative_Duration  Informational  \\\n",
       "0               0                      0.0              0   \n",
       "1               0                      0.0              0   \n",
       "2               0                      0.0              0   \n",
       "3               0                      0.0              0   \n",
       "4               0                      0.0              0   \n",
       "\n",
       "   Informational_Duration  ProductRelated  ProductRelated_Duration  \\\n",
       "0                     0.0               1                 0.000000   \n",
       "1                     0.0               2                64.000000   \n",
       "2                     0.0               1                 0.000000   \n",
       "3                     0.0               2                 2.666667   \n",
       "4                     0.0              10               627.500000   \n",
       "\n",
       "   BounceRates  ExitRates  PageValues  SpecialDay Month  OperatingSystems  \\\n",
       "0         0.20       0.20         0.0         0.0   Feb                 1   \n",
       "1         0.00       0.10         0.0         0.0   Feb                 2   \n",
       "2         0.20       0.20         0.0         0.0   Feb                 4   \n",
       "3         0.05       0.14         0.0         0.0   Feb                 3   \n",
       "4         0.02       0.05         0.0         0.0   Feb                 3   \n",
       "\n",
       "   Browser  Region  TrafficType        VisitorType  Weekend  Revenue  \n",
       "0        1       1            1  Returning_Visitor    False    False  \n",
       "1        2       1            2  Returning_Visitor    False    False  \n",
       "2        1       9            3  Returning_Visitor    False    False  \n",
       "3        2       2            4  Returning_Visitor    False    False  \n",
       "4        3       1            4  Returning_Visitor     True    False  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "myData=pd.read_csv('imputedData.csv')\n",
    "myData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12330 entries, 0 to 12329\n",
      "Data columns (total 57 columns):\n",
      "Administrative                   12330 non-null int64\n",
      "Administrative_Duration          12330 non-null float64\n",
      "Informational                    12330 non-null int64\n",
      "Informational_Duration           12330 non-null float64\n",
      "ProductRelated                   12330 non-null int64\n",
      "ProductRelated_Duration          12330 non-null float64\n",
      "BounceRates                      12330 non-null float64\n",
      "ExitRates                        12330 non-null float64\n",
      "PageValues                       12330 non-null float64\n",
      "TrafficType                      12330 non-null int64\n",
      "SpecialDay                       12330 non-null float64\n",
      "Month_Aug                        12330 non-null uint8\n",
      "Month_Dec                        12330 non-null uint8\n",
      "Month_Feb                        12330 non-null uint8\n",
      "Month_Jul                        12330 non-null uint8\n",
      "Month_June                       12330 non-null uint8\n",
      "Month_Mar                        12330 non-null uint8\n",
      "Month_May                        12330 non-null uint8\n",
      "Month_Nov                        12330 non-null uint8\n",
      "Month_Oct                        12330 non-null uint8\n",
      "Month_Sep                        12330 non-null uint8\n",
      "OperatingSystems_1               12330 non-null uint8\n",
      "OperatingSystems_2               12330 non-null uint8\n",
      "OperatingSystems_3               12330 non-null uint8\n",
      "OperatingSystems_4               12330 non-null uint8\n",
      "OperatingSystems_5               12330 non-null uint8\n",
      "OperatingSystems_6               12330 non-null uint8\n",
      "OperatingSystems_7               12330 non-null uint8\n",
      "OperatingSystems_8               12330 non-null uint8\n",
      "Browser_1                        12330 non-null uint8\n",
      "Browser_2                        12330 non-null uint8\n",
      "Browser_3                        12330 non-null uint8\n",
      "Browser_4                        12330 non-null uint8\n",
      "Browser_5                        12330 non-null uint8\n",
      "Browser_6                        12330 non-null uint8\n",
      "Browser_7                        12330 non-null uint8\n",
      "Browser_8                        12330 non-null uint8\n",
      "Browser_9                        12330 non-null uint8\n",
      "Browser_10                       12330 non-null uint8\n",
      "Browser_11                       12330 non-null uint8\n",
      "Browser_12                       12330 non-null uint8\n",
      "Browser_13                       12330 non-null uint8\n",
      "Region_1                         12330 non-null uint8\n",
      "Region_2                         12330 non-null uint8\n",
      "Region_3                         12330 non-null uint8\n",
      "Region_4                         12330 non-null uint8\n",
      "Region_5                         12330 non-null uint8\n",
      "Region_6                         12330 non-null uint8\n",
      "Region_7                         12330 non-null uint8\n",
      "Region_8                         12330 non-null uint8\n",
      "Region_9                         12330 non-null uint8\n",
      "VisitorType_New_Visitor          12330 non-null uint8\n",
      "VisitorType_Other                12330 non-null uint8\n",
      "VisitorType_Returning_Visitor    12330 non-null uint8\n",
      "Weekend_False                    12330 non-null uint8\n",
      "Weekend_True                     12330 non-null uint8\n",
      "Revenue                          12330 non-null bool\n",
      "dtypes: bool(1), float64(7), int64(4), uint8(45)\n",
      "memory usage: 1.6 MB\n"
     ]
    }
   ],
   "source": [
    "#One hot encoding of categorical variables\n",
    "\n",
    "#Create list with features to be dummified cols.\n",
    "nonum_feats_names = ['Month','OperatingSystems','Browser','Region','VisitorType','Weekend']\n",
    "\n",
    "\n",
    "#Boolean to dummify logic\n",
    "#Weekend_map={False:0,True:1}\n",
    "#pd.get_dummies(myData['Weekend'].map(Weekend_map).astype('category')\n",
    "\n",
    "\n",
    "dataModel = pd.concat([myData[['Administrative', 'Administrative_Duration', 'Informational','Informational_Duration', \n",
    "                              'ProductRelated','ProductRelated_Duration','BounceRates','ExitRates','PageValues',\n",
    "                               'TrafficType','SpecialDay']],\n",
    "                       pd.get_dummies(myData[nonum_feats_names].astype('category')),myData['Revenue']],axis=1)\n",
    "                      \n",
    "dataModel.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "Name: Revenue, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Label Encoding of revenue\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "myData['Revenue'] = le.fit_transform(myData['Revenue'])\n",
    "myData['Revenue'].value_counts()\n",
    "\n",
    "myData['Revenue'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x: (12330, 56)\n",
      "Shape of y: (12330,)\n"
     ]
    }
   ],
   "source": [
    "# getting dependent and independent variables\n",
    "\n",
    "x = dataModel\n",
    "# removing the target column revenue from x\n",
    "x = x.drop(['Revenue'], axis = 1)\n",
    "\n",
    "y = myData['Revenue']\n",
    "\n",
    "# checking the shapes\n",
    "print(\"Shape of x:\", x.shape)\n",
    "print(\"Shape of y:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train : (8631, 56)\n",
      "Shape of y_train : (8631,)\n",
      "Shape of x_test : (3699, 56)\n",
      "Shape of y_test : (3699,)\n"
     ]
    }
   ],
   "source": [
    "# splitting the data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_baseTrain, x_baseTest, y_baseTrain, y_baseTest = train_test_split(x, y, test_size = 0.3, random_state = 42)\n",
    "\n",
    "# checking the shapes\n",
    "\n",
    "print(\"Shape of x_train :\", x_baseTrain.shape)\n",
    "print(\"Shape of y_train :\", y_baseTrain.shape)\n",
    "print(\"Shape of x_test :\", x_baseTest.shape)\n",
    "print(\"Shape of y_test :\", y_baseTest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy : 0.8839068474104971\n",
      "Testing Accuracy : 0.881859962151933\n",
      "ROC AUC Score : 0.6682469520681401\n",
      "[[3056   68]\n",
      " [ 369  206]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.98      0.93      3124\n",
      "           1       0.75      0.36      0.49       575\n",
      "\n",
      "    accuracy                           0.88      3699\n",
      "   macro avg       0.82      0.67      0.71      3699\n",
      "weighted avg       0.87      0.88      0.86      3699\n",
      "\n",
      "{'fit_time': array([0.17054415, 0.09175396, 0.12466621, 0.12267137, 0.07978654,\n",
      "       0.09574342, 0.10372186, 0.10870957, 0.14361548, 0.10272551]), 'score_time': array([0.00698185, 0.00698185, 0.00598431, 0.00598431, 0.00598407,\n",
      "       0.00598431, 0.00598407, 0.00698161, 0.00698185, 0.00598359]), 'test_accuracy': array([0.88194444, 0.87152778, 0.88078704, 0.88760139, 0.89455388,\n",
      "       0.88644264, 0.87485516, 0.89107764, 0.88167053, 0.88283063]), 'test_precision': array([0.75806452, 0.71698113, 0.73846154, 0.75714286, 0.8       ,\n",
      "       0.73972603, 0.66233766, 0.73493976, 0.75409836, 0.72857143]), 'test_recall': array([0.35074627, 0.28358209, 0.35820896, 0.39849624, 0.42105263,\n",
      "       0.40601504, 0.38345865, 0.45864662, 0.34586466, 0.38345865]), 'test_f1_score': array([0.47959184, 0.40641711, 0.48241206, 0.52216749, 0.55172414,\n",
      "       0.52427184, 0.48571429, 0.56481481, 0.4742268 , 0.50246305])}\n",
      "Mean Accuracy : 0.8833291126188684\n",
      "Mean Standard Deviation : 0.00660635866310523\n",
      "Mean precision score : 0.7390323281807134\n",
      "Mean Standard Deviation precision score : 0.03336101221453834\n",
      "Mean recall score : 0.3789529794635843\n",
      "Mean Standard Deviation recall score : 0.04549145976136961\n",
      "Mean f1 score : 0.49938034384516283\n",
      "Mean Standard Deviation f1 score : 0.04279137576133549\n"
     ]
    }
   ],
   "source": [
    "#Scaling, Feature Selection not done yet.\n",
    "#Correlation not yet considered.\n",
    "# MODELLING ____ LOGISTIC REGRESSION\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(x_baseTrain, y_baseTrain)\n",
    "\n",
    "y_basePred = model.predict(x_baseTest)\n",
    "\n",
    "# evaluating the model\n",
    "print(\"Training Accuracy :\", model.score(x_baseTrain, y_baseTrain))\n",
    "print(\"Testing Accuracy :\", model.score(x_baseTest, y_baseTest))\n",
    "print(\"ROC AUC Score :\", roc_auc_score(y_baseTest, y_basePred))\n",
    "\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(y_baseTest, y_basePred)\n",
    "print(cm)\n",
    "#plt.rcParams['figure.figsize'] = (6, 6)\n",
    "#sns.heatmap(cm ,annot = True)\n",
    "\n",
    "# classification report\n",
    "cr = classification_report(y_baseTest, y_basePred)\n",
    "print(cr)\n",
    "\n",
    "\n",
    "# cross validation\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "scoring = {'accuracy' : make_scorer(accuracy_score), \n",
    "           'precision' : make_scorer(precision_score),\n",
    "           'recall' : make_scorer(recall_score), \n",
    "           'f1_score' : make_scorer(f1_score)}\n",
    "cvs = cross_validate(estimator = model, X = x_baseTrain, y = y_baseTrain, cv = 10,scoring=scoring)\n",
    "print(cvs)\n",
    "print(\"Mean Accuracy :\", np.mean(cvs['test_accuracy']))\n",
    "print(\"Mean Standard Deviation :\", np.std(cvs['test_accuracy']))\n",
    "print(\"Mean precision score :\", np.mean(cvs['test_precision']))\n",
    "print(\"Mean Standard Deviation precision score :\", np.std(cvs['test_precision']))\n",
    "print(\"Mean recall score :\", np.mean(cvs['test_recall']))\n",
    "print(\"Mean Standard Deviation recall score :\", np.std(cvs['test_recall']))\n",
    "print(\"Mean f1 score :\", np.mean(cvs['test_f1_score']))\n",
    "print(\"Mean Standard Deviation f1 score :\", np.std(cvs['test_f1_score']))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy : 0.9888773027459159\n",
      "Testing Accuracy : 0.8913219789132197\n",
      "ROC AUC Score : 0.7199671547068975\n",
      "[[3026   98]\n",
      " [ 304  271]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.97      0.94      3124\n",
      "           1       0.73      0.47      0.57       575\n",
      "\n",
      "    accuracy                           0.89      3699\n",
      "   macro avg       0.82      0.72      0.76      3699\n",
      "weighted avg       0.88      0.89      0.88      3699\n",
      "\n",
      "{'fit_time': array([0.08776569, 0.07479978, 0.07776427, 0.07679367, 0.07679439,\n",
      "       0.07382894, 0.07577014, 0.07081008, 0.07679415, 0.07884479]), 'score_time': array([0.01398921, 0.0139637 , 0.01399016, 0.01393652, 0.01396322,\n",
      "       0.01396275, 0.01296592, 0.01296592, 0.01396298, 0.01295114]), 'test_accuracy': array([0.89467593, 0.87615741, 0.90625   , 0.90961761, 0.89687138,\n",
      "       0.90266512, 0.88644264, 0.89687138, 0.89559165, 0.88399072]), 'test_precision': array([0.74712644, 0.67088608, 0.8045977 , 0.81609195, 0.73404255,\n",
      "       0.73333333, 0.73972603, 0.73913043, 0.8115942 , 0.7037037 ]), 'test_recall': array([0.48507463, 0.39552239, 0.52238806, 0.53383459, 0.51879699,\n",
      "       0.57894737, 0.40601504, 0.5112782 , 0.42105263, 0.42857143]), 'test_f1_score': array([0.58823529, 0.49765258, 0.63348416, 0.64545455, 0.60792952,\n",
      "       0.64705882, 0.52427184, 0.60444444, 0.55445545, 0.53271028])}\n",
      "Mean Accuracy : 0.8949133834337493\n",
      "Mean Standard Deviation : 0.0097767647953107\n",
      "Mean precision score : 0.7500232423210337\n",
      "Mean Standard Deviation precision score : 0.04506621021178374\n",
      "Mean recall score : 0.48014813152283703\n",
      "Mean Standard Deviation recall score : 0.05976996599794952\n",
      "Mean f1 score : 0.5835696938598682\n",
      "Mean Standard Deviation f1 score : 0.05072995860837463\n"
     ]
    }
   ],
   "source": [
    "# MODELLING ____RANDOM FOREST\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "model = RandomForestClassifier()  #can try estimators=200,mex depth=30\n",
    "model.fit(x_baseTrain, y_baseTrain)\n",
    "\n",
    "y_basePred = model.predict(x_baseTest)\n",
    "\n",
    "# evaluating the model\n",
    "print(\"Training Accuracy :\", model.score(x_baseTrain, y_baseTrain))\n",
    "print(\"Testing Accuracy :\", model.score(x_baseTest, y_baseTest))\n",
    "print(\"ROC AUC Score :\", roc_auc_score(y_baseTest, y_basePred))\n",
    "\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(y_baseTest, y_basePred)\n",
    "print(cm)\n",
    "\n",
    "# classification report\n",
    "cr = classification_report(y_baseTest, y_basePred)\n",
    "print(cr)\n",
    "\n",
    "\n",
    "# cross validation\n",
    "scoring = {'accuracy' : make_scorer(accuracy_score), \n",
    "           'precision' : make_scorer(precision_score),\n",
    "           'recall' : make_scorer(recall_score), \n",
    "           'f1_score' : make_scorer(f1_score)}\n",
    "cvs = cross_validate(estimator = model, X = x_baseTrain, y = y_baseTrain, cv = 10,scoring=scoring)\n",
    "print(cvs)\n",
    "print(\"Mean Accuracy :\", np.mean(cvs['test_accuracy']))\n",
    "print(\"Mean Standard Deviation :\", np.std(cvs['test_accuracy']))\n",
    "print(\"Mean precision score :\", np.mean(cvs['test_precision']))\n",
    "print(\"Mean Standard Deviation precision score :\", np.std(cvs['test_precision']))\n",
    "print(\"Mean recall score :\", np.mean(cvs['test_recall']))\n",
    "print(\"Mean Standard Deviation recall score :\", np.std(cvs['test_recall']))\n",
    "print(\"Mean f1 score :\", np.mean(cvs['test_f1_score']))\n",
    "print(\"Mean Standard Deviation f1 score :\", np.std(cvs['test_f1_score']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy : 1.0\n",
      "Testing Accuracy : 0.8572587185725872\n",
      "ROC AUC Score : 0.7359859154929578\n",
      "[[2849  275]\n",
      " [ 253  322]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.92      3124\n",
      "           1       0.54      0.56      0.55       575\n",
      "\n",
      "    accuracy                           0.86      3699\n",
      "   macro avg       0.73      0.74      0.73      3699\n",
      "weighted avg       0.86      0.86      0.86      3699\n",
      "\n",
      "{'fit_time': array([0.05585027, 0.06482625, 0.05884194, 0.05884266, 0.05587697,\n",
      "       0.05884242, 0.05684876, 0.05884194, 0.058846  , 0.05884218]), 'score_time': array([0.00698185, 0.00598454, 0.00598431, 0.00698113, 0.00598383,\n",
      "       0.00698209, 0.00698018, 0.00698256, 0.00597978, 0.00598669]), 'test_accuracy': array([0.875     , 0.84490741, 0.85532407, 0.8783314 , 0.85863268,\n",
      "       0.85979143, 0.86558517, 0.83429896, 0.86542923, 0.8549884 ]), 'test_precision': array([0.59285714, 0.5       , 0.53333333, 0.60294118, 0.53846154,\n",
      "       0.54347826, 0.5620438 , 0.46527778, 0.57391304, 0.52739726]), 'test_recall': array([0.61940299, 0.52985075, 0.53731343, 0.61654135, 0.57894737,\n",
      "       0.56390977, 0.57894737, 0.5037594 , 0.4962406 , 0.57894737]), 'test_f1_score': array([0.60583942, 0.51449275, 0.53531599, 0.60966543, 0.55797101,\n",
      "       0.55350554, 0.57037037, 0.48375451, 0.53225806, 0.55197133])}\n",
      "Mean Accuracy : 0.8592288744092619\n",
      "Mean Standard Deviation : 0.012461894328130534\n",
      "Mean precision score : 0.5439703329142618\n",
      "Mean Standard Deviation precision score : 0.039541125938434216\n",
      "Mean recall score : 0.5603860397261811\n",
      "Mean Standard Deviation recall score : 0.0405812366029632\n",
      "Mean f1 score : 0.5515144405555845\n",
      "Mean Standard Deviation f1 score : 0.03650939330698372\n"
     ]
    }
   ],
   "source": [
    "# MODELLING ____DECISION TREE\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(x_baseTrain, y_baseTrain)\n",
    "\n",
    "y_basePred = model.predict(x_baseTest)\n",
    "\n",
    "# evaluating the model\n",
    "print(\"Training Accuracy :\", model.score(x_baseTrain, y_baseTrain))\n",
    "print(\"Testing Accuracy :\", model.score(x_baseTest, y_baseTest))\n",
    "print(\"ROC AUC Score :\", roc_auc_score(y_baseTest, y_basePred))\n",
    "\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(y_baseTest, y_basePred)\n",
    "print(cm)\n",
    "\n",
    "# classification report\n",
    "cr = classification_report(y_baseTest, y_basePred)\n",
    "print(cr)\n",
    "\n",
    "\n",
    "# cross validation\n",
    "scoring = {'accuracy' : make_scorer(accuracy_score), \n",
    "           'precision' : make_scorer(precision_score),\n",
    "           'recall' : make_scorer(recall_score), \n",
    "           'f1_score' : make_scorer(f1_score)}\n",
    "cvs = cross_validate(estimator = model, X = x_baseTrain, y = y_baseTrain, cv = 10,scoring=scoring)\n",
    "print(cvs)\n",
    "print(\"Mean Accuracy :\", np.mean(cvs['test_accuracy']))\n",
    "print(\"Mean Standard Deviation :\", np.std(cvs['test_accuracy']))\n",
    "print(\"Mean precision score :\", np.mean(cvs['test_precision']))\n",
    "print(\"Mean Standard Deviation precision score :\", np.std(cvs['test_precision']))\n",
    "print(\"Mean recall score :\", np.mean(cvs['test_recall']))\n",
    "print(\"Mean Standard Deviation recall score :\", np.std(cvs['test_recall']))\n",
    "print(\"Mean f1 score :\", np.mean(cvs['test_f1_score']))\n",
    "print(\"Mean Standard Deviation f1 score :\", np.std(cvs['test_f1_score']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy : 0.7670026648128838\n",
      "Testing Accuracy : 0.7742633144092999\n",
      "ROC AUC Score : 0.7336781161275956\n",
      "[[2476  648]\n",
      " [ 187  388]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.79      0.86      3124\n",
      "           1       0.37      0.67      0.48       575\n",
      "\n",
      "    accuracy                           0.77      3699\n",
      "   macro avg       0.65      0.73      0.67      3699\n",
      "weighted avg       0.84      0.77      0.80      3699\n",
      "\n",
      "{'fit_time': array([0.01495981, 0.01496077, 0.01495981, 0.01495934, 0.01495957,\n",
      "       0.01695585, 0.01695395, 0.01495934, 0.01595783, 0.01396275]), 'score_time': array([0.00797844, 0.00797868, 0.00797939, 0.00797915, 0.00797844,\n",
      "       0.00897598, 0.00797939, 0.00797868, 0.00797844, 0.00797844]), 'test_accuracy': array([0.76388889, 0.74305556, 0.78240741, 0.77056779, 0.78563152,\n",
      "       0.77867903, 0.73812283, 0.74739282, 0.76566125, 0.75986079]), 'test_precision': array([0.33796296, 0.33206107, 0.37837838, 0.36514523, 0.3907563 ,\n",
      "       0.37916667, 0.31474104, 0.34875445, 0.35319149, 0.35546875]), 'test_recall': array([0.54477612, 0.64925373, 0.62686567, 0.66165414, 0.69924812,\n",
      "       0.68421053, 0.59398496, 0.73684211, 0.62406015, 0.68421053]), 'test_f1_score': array([0.41714286, 0.43939394, 0.47191011, 0.47058824, 0.50134771,\n",
      "       0.48793566, 0.41145833, 0.47342995, 0.45108696, 0.46786632])}\n",
      "Mean Accuracy : 0.7635267868122725\n",
      "Mean Standard Deviation : 0.015707091131196123\n",
      "Mean precision score : 0.35556263310639263\n",
      "Mean Standard Deviation precision score : 0.02233514352956275\n",
      "Mean recall score : 0.6505106048703849\n",
      "Mean Standard Deviation recall score : 0.05278185281610709\n",
      "Mean f1 score : 0.4592160075375153\n",
      "Mean Standard Deviation f1 score : 0.027707709836117207\n"
     ]
    }
   ],
   "source": [
    "# MODELLING ____NAIVE BAYES\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "model = GaussianNB()\n",
    "model.fit(x_baseTrain, y_baseTrain)\n",
    "\n",
    "y_basePred = model.predict(x_baseTest)\n",
    "\n",
    "# evaluating the model\n",
    "print(\"Training Accuracy :\", model.score(x_baseTrain, y_baseTrain))\n",
    "print(\"Testing Accuracy :\", model.score(x_baseTest, y_baseTest))\n",
    "print(\"ROC AUC Score :\", roc_auc_score(y_baseTest, y_basePred))\n",
    "\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(y_baseTest, y_basePred)\n",
    "print(cm)\n",
    "\n",
    "# classification report\n",
    "cr = classification_report(y_baseTest, y_basePred)\n",
    "print(cr)\n",
    "\n",
    "\n",
    "# cross validation\n",
    "scoring = {'accuracy' : make_scorer(accuracy_score), \n",
    "           'precision' : make_scorer(precision_score),\n",
    "           'recall' : make_scorer(recall_score), \n",
    "           'f1_score' : make_scorer(f1_score)}\n",
    "cvs = cross_validate(estimator = model, X = x_baseTrain, y = y_baseTrain, cv = 10,scoring=scoring)\n",
    "print(cvs)\n",
    "print(\"Mean Accuracy :\", np.mean(cvs['test_accuracy']))\n",
    "print(\"Mean Standard Deviation :\", np.std(cvs['test_accuracy']))\n",
    "print(\"Mean precision score :\", np.mean(cvs['test_precision']))\n",
    "print(\"Mean Standard Deviation precision score :\", np.std(cvs['test_precision']))\n",
    "print(\"Mean recall score :\", np.mean(cvs['test_recall']))\n",
    "print(\"Mean Standard Deviation recall score :\", np.std(cvs['test_recall']))\n",
    "print(\"Mean f1 score :\", np.mean(cvs['test_f1_score']))\n",
    "print(\"Mean Standard Deviation f1 score :\", np.std(cvs['test_f1_score']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy : 0.8979260804078323\n",
      "Testing Accuracy : 0.8532035685320357\n",
      "ROC AUC Score : 0.6150963090797751\n",
      "[[3001  123]\n",
      " [ 420  155]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.96      0.92      3124\n",
      "           1       0.56      0.27      0.36       575\n",
      "\n",
      "    accuracy                           0.85      3699\n",
      "   macro avg       0.72      0.62      0.64      3699\n",
      "weighted avg       0.83      0.85      0.83      3699\n",
      "\n",
      "{'fit_time': array([0.04188728, 0.0369277 , 0.03989315, 0.03094411, 0.03593063,\n",
      "       0.03590202, 0.03690124, 0.03593159, 0.03593111, 0.03590369]), 'score_time': array([0.24936485, 0.2403574 , 0.24337602, 0.22639394, 0.23337674,\n",
      "       0.24035764, 0.23237896, 0.24035597, 0.2443459 , 0.23138142]), 'test_accuracy': array([0.86111111, 0.84606481, 0.86574074, 0.87137891, 0.87253766,\n",
      "       0.86442642, 0.85515643, 0.86906141, 0.87703016, 0.87238979]), 'test_precision': array([0.62068966, 0.50847458, 0.65      , 0.671875  , 0.66666667,\n",
      "       0.61428571, 0.5625    , 0.63513514, 0.69565217, 0.66666667]), 'test_recall': array([0.26865672, 0.2238806 , 0.29104478, 0.32330827, 0.34586466,\n",
      "       0.32330827, 0.27067669, 0.35338346, 0.36090226, 0.34586466]), 'test_f1_score': array([0.375     , 0.31088083, 0.40206186, 0.43654822, 0.45544554,\n",
      "       0.42364532, 0.36548223, 0.45410628, 0.47524752, 0.45544554])}\n",
      "Mean Accuracy : 0.8654897454561912\n",
      "Mean Standard Deviation : 0.008845198940694764\n",
      "Mean precision score : 0.6291945588110827\n",
      "Mean Standard Deviation precision score : 0.05369431498767061\n",
      "Mean recall score : 0.31068903602289305\n",
      "Mean Standard Deviation recall score : 0.042926246293370066\n",
      "Mean f1 score : 0.4153863355790107\n",
      "Mean Standard Deviation f1 score : 0.049018905558079785\n"
     ]
    }
   ],
   "source": [
    "# MODELLING ____KNN\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "model = KNeighborsClassifier()\n",
    "model.fit(x_baseTrain, y_baseTrain)\n",
    "\n",
    "y_basePred = model.predict(x_baseTest)\n",
    "\n",
    "# evaluating the model\n",
    "print(\"Training Accuracy :\", model.score(x_baseTrain, y_baseTrain))\n",
    "print(\"Testing Accuracy :\", model.score(x_baseTest, y_baseTest))\n",
    "print(\"ROC AUC Score :\", roc_auc_score(y_baseTest, y_basePred))\n",
    "\n",
    "\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(y_baseTest, y_basePred)\n",
    "print(cm)\n",
    "\n",
    "# classification report\n",
    "cr = classification_report(y_baseTest, y_basePred)\n",
    "print(cr)\n",
    "\n",
    "\n",
    "# cross validation\n",
    "scoring = {'accuracy' : make_scorer(accuracy_score), \n",
    "           'precision' : make_scorer(precision_score),\n",
    "           'recall' : make_scorer(recall_score), \n",
    "           'f1_score' : make_scorer(f1_score)}\n",
    "cvs = cross_validate(estimator = model, X = x_baseTrain, y = y_baseTrain, cv = 10,scoring=scoring)\n",
    "print(cvs)\n",
    "print(\"Mean Accuracy :\", np.mean(cvs['test_accuracy']))\n",
    "print(\"Mean Standard Deviation :\", np.std(cvs['test_accuracy']))\n",
    "print(\"Mean precision score :\", np.mean(cvs['test_precision']))\n",
    "print(\"Mean Standard Deviation precision score :\", np.std(cvs['test_precision']))\n",
    "print(\"Mean recall score :\", np.mean(cvs['test_recall']))\n",
    "print(\"Mean Standard Deviation recall score :\", np.std(cvs['test_recall']))\n",
    "print(\"Mean f1 score :\", np.mean(cvs['test_f1_score']))\n",
    "print(\"Mean Standard Deviation f1 score :\", np.std(cvs['test_f1_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy : 0.7521724018074383\n",
      "Testing Accuracy : 0.7585834009191673\n",
      "ROC AUC Score : 0.75561376162111\n",
      "[[2374  750]\n",
      " [ 143  432]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.76      0.84      3124\n",
      "           1       0.37      0.75      0.49       575\n",
      "\n",
      "    accuracy                           0.76      3699\n",
      "   macro avg       0.65      0.76      0.67      3699\n",
      "weighted avg       0.85      0.76      0.79      3699\n",
      "\n",
      "{'fit_time': array([0.68317318, 0.65536523, 0.67899942, 0.66322565, 0.65524697,\n",
      "       0.66023421, 0.66325355, 0.6582396 , 0.65824008, 0.66724253]), 'score_time': array([0.00698161, 0.        , 0.00698233, 0.00698185, 0.00698137,\n",
      "       0.00598454, 0.0059557 , 0.00698137, 0.00598359, 0.00595713]), 'test_accuracy': array([0.52314815, 0.86458333, 0.82986111, 0.87485516, 0.85747393,\n",
      "       0.8424102 , 0.85399768, 0.71147161, 0.86426914, 0.79118329]), 'test_precision': array([0.23371648, 0.69767442, 0.45255474, 0.60683761, 0.75      ,\n",
      "       0.46511628, 0.5443038 , 0.29285714, 0.9       , 0.2920354 ]), 'test_recall': array([0.91044776, 0.2238806 , 0.46268657, 0.53383459, 0.11278195,\n",
      "       0.15037594, 0.32330827, 0.61654135, 0.13533835, 0.2481203 ]), 'test_f1_score': array([0.37195122, 0.33898305, 0.45756458, 0.568     , 0.19607843,\n",
      "       0.22727273, 0.40566038, 0.39709443, 0.23529412, 0.26829268])}\n",
      "Mean Accuracy : 0.8013253603526771\n",
      "Mean Standard Deviation : 0.10361396089618599\n",
      "Mean precision score : 0.5235095862688944\n",
      "Mean Standard Deviation precision score : 0.20735417785838242\n",
      "Mean recall score : 0.3717315677252834\n",
      "Mean Standard Deviation recall score : 0.24415499395874982\n",
      "Mean f1 score : 0.34661916135758003\n",
      "Mean Standard Deviation f1 score : 0.111124429064443\n"
     ]
    }
   ],
   "source": [
    "# MODELLING ____Linear SVC\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "model = LinearSVC()\n",
    "model.fit(x_baseTrain, y_baseTrain)\n",
    "\n",
    "y_basePred = model.predict(x_baseTest)\n",
    "\n",
    "# evaluating the model\n",
    "print(\"Training Accuracy :\", model.score(x_baseTrain, y_baseTrain))\n",
    "print(\"Testing Accuracy :\", model.score(x_baseTest, y_baseTest))\n",
    "print(\"ROC AUC Score :\", roc_auc_score(y_baseTest, y_basePred))\n",
    "\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(y_baseTest, y_basePred)\n",
    "print(cm)\n",
    "\n",
    "# classification report\n",
    "cr = classification_report(y_baseTest, y_basePred)\n",
    "print(cr)\n",
    "\n",
    "\n",
    "# cross validation\n",
    "scoring = {'accuracy' : make_scorer(accuracy_score), \n",
    "           'precision' : make_scorer(precision_score),\n",
    "           'recall' : make_scorer(recall_score), \n",
    "           'f1_score' : make_scorer(f1_score)}\n",
    "cvs = cross_validate(estimator = model, X = x_baseTrain, y = y_baseTrain, cv = 10,scoring=scoring)\n",
    "print(cvs)\n",
    "print(\"Mean Accuracy :\", np.mean(cvs['test_accuracy']))\n",
    "print(\"Mean Standard Deviation :\", np.std(cvs['test_accuracy']))\n",
    "print(\"Mean precision score :\", np.mean(cvs['test_precision']))\n",
    "print(\"Mean Standard Deviation precision score :\", np.std(cvs['test_precision']))\n",
    "print(\"Mean recall score :\", np.mean(cvs['test_recall']))\n",
    "print(\"Mean Standard Deviation recall score :\", np.std(cvs['test_recall']))\n",
    "print(\"Mean f1 score :\", np.mean(cvs['test_f1_score']))\n",
    "print(\"Mean Standard Deviation f1 score :\", np.std(cvs['test_f1_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy : 0.8983895261267524\n",
      "Testing Accuracy : 0.8888888888888888\n",
      "ROC AUC Score : 0.750454823804487\n",
      "[[2972  152]\n",
      " [ 259  316]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.94      3124\n",
      "           1       0.68      0.55      0.61       575\n",
      "\n",
      "    accuracy                           0.89      3699\n",
      "   macro avg       0.80      0.75      0.77      3699\n",
      "weighted avg       0.88      0.89      0.88      3699\n",
      "\n",
      "{'fit_time': array([0.53856015, 0.4996357 , 0.51659203, 0.50063515, 0.49863935,\n",
      "       0.49368143, 0.49864006, 0.49764323, 0.50063682, 0.49265599]), 'score_time': array([0.05587769, 0.05388331, 0.0568738 , 0.05886889, 0.05388284,\n",
      "       0.05385351, 0.05288553, 0.0528841 , 0.05388045, 0.0538826 ]), 'test_accuracy': array([0.90046296, 0.87384259, 0.88657407, 0.90382387, 0.91772885,\n",
      "       0.89455388, 0.8783314 , 0.88412514, 0.90023202, 0.88863109]), 'test_precision': array([0.73076923, 0.62626263, 0.66363636, 0.73148148, 0.76271186,\n",
      "       0.68421053, 0.6147541 , 0.62222222, 0.72380952, 0.66666667]), 'test_recall': array([0.56716418, 0.46268657, 0.54477612, 0.59398496, 0.67669173,\n",
      "       0.58646617, 0.56390977, 0.63157895, 0.57142857, 0.55639098]), 'test_f1_score': array([0.63865546, 0.53218884, 0.59836066, 0.65560166, 0.71713147,\n",
      "       0.63157895, 0.58823529, 0.62686567, 0.63865546, 0.60655738])}\n",
      "Mean Accuracy : 0.8928305890474414\n",
      "Mean Standard Deviation : 0.012490455386508803\n",
      "Mean precision score : 0.682652460393134\n",
      "Mean Standard Deviation precision score : 0.04986412406094516\n",
      "Mean recall score : 0.5755077993491191\n",
      "Mean Standard Deviation recall score : 0.053012208416138885\n",
      "Mean f1 score : 0.6233830845340833\n",
      "Mean Standard Deviation f1 score : 0.04564380825262331\n"
     ]
    }
   ],
   "source": [
    "# MODELLING ____Ada Boost Classifier\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "\n",
    "model = AdaBoostClassifier()\n",
    "model.fit(x_baseTrain, y_baseTrain)\n",
    "\n",
    "y_basePred = model.predict(x_baseTest)\n",
    "\n",
    "# evaluating the model\n",
    "print(\"Training Accuracy :\", model.score(x_baseTrain, y_baseTrain))\n",
    "print(\"Testing Accuracy :\", model.score(x_baseTest, y_baseTest))\n",
    "print(\"ROC AUC Score :\", roc_auc_score(y_baseTest, y_basePred))\n",
    "\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(y_baseTest, y_basePred)\n",
    "print(cm)\n",
    "\n",
    "# classification report\n",
    "cr = classification_report(y_baseTest, y_basePred)\n",
    "print(cr)\n",
    "\n",
    "\n",
    "# cross validation\n",
    "scoring = {'accuracy' : make_scorer(accuracy_score), \n",
    "           'precision' : make_scorer(precision_score),\n",
    "           'recall' : make_scorer(recall_score), \n",
    "           'f1_score' : make_scorer(f1_score)}\n",
    "cvs = cross_validate(estimator = model, X = x_baseTrain, y = y_baseTrain, cv = 10,scoring=scoring)\n",
    "print(cvs)\n",
    "print(\"Mean Accuracy :\", np.mean(cvs['test_accuracy']))\n",
    "print(\"Mean Standard Deviation :\", np.std(cvs['test_accuracy']))\n",
    "print(\"Mean precision score :\", np.mean(cvs['test_precision']))\n",
    "print(\"Mean Standard Deviation precision score :\", np.std(cvs['test_precision']))\n",
    "print(\"Mean recall score :\", np.mean(cvs['test_recall']))\n",
    "print(\"Mean Standard Deviation recall score :\", np.std(cvs['test_recall']))\n",
    "print(\"Mean f1 score :\", np.mean(cvs['test_f1_score']))\n",
    "print(\"Mean Standard Deviation f1 score :\", np.std(cvs['test_f1_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy : 0.9210983663538408\n",
      "Testing Accuracy : 0.8991619356582861\n",
      "ROC AUC Score : 0.7785317040583422\n",
      "[[2979  145]\n",
      " [ 228  347]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.95      0.94      3124\n",
      "           1       0.71      0.60      0.65       575\n",
      "\n",
      "    accuracy                           0.90      3699\n",
      "   macro avg       0.82      0.78      0.80      3699\n",
      "weighted avg       0.89      0.90      0.90      3699\n",
      "\n",
      "{'fit_time': array([0.85870337, 0.82581878, 0.82282615, 0.83678865, 0.81983471,\n",
      "       0.79886341, 0.79487371, 0.80085826, 0.77997565, 0.81316161]), 'score_time': array([0.01396322, 0.01795983, 0.01496029, 0.01396275, 0.01495957,\n",
      "       0.01396322, 0.01396298, 0.01396275, 0.01561975, 0.01596069]), 'test_accuracy': array([0.91203704, 0.8900463 , 0.90972222, 0.92468134, 0.9212051 ,\n",
      "       0.91425261, 0.89918888, 0.91193511, 0.90603248, 0.90487239]), 'test_precision': array([0.75892857, 0.68224299, 0.75      , 0.80357143, 0.78761062,\n",
      "       0.76576577, 0.69827586, 0.7394958 , 0.76530612, 0.73394495]), 'test_recall': array([0.63432836, 0.54477612, 0.62686567, 0.67669173, 0.66917293,\n",
      "       0.63909774, 0.60902256, 0.66165414, 0.56390977, 0.60150376]), 'test_f1_score': array([0.69105691, 0.60580913, 0.68292683, 0.73469388, 0.72357724,\n",
      "       0.69672131, 0.65060241, 0.6984127 , 0.64935065, 0.66115702])}\n",
      "Mean Accuracy : 0.9093973463866553\n",
      "Mean Standard Deviation : 0.00959470926016993\n",
      "Mean precision score : 0.7485142112854711\n",
      "Mean Standard Deviation precision score : 0.035297930007899186\n",
      "Mean recall score : 0.6227022780832678\n",
      "Mean Standard Deviation recall score : 0.04144695080010724\n",
      "Mean f1 score : 0.6794308075462181\n",
      "Mean Standard Deviation f1 score : 0.036468549961964865\n"
     ]
    }
   ],
   "source": [
    "# MODELLING ____XGBOOST classifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "\n",
    "model = XGBClassifier()\n",
    "model.fit(x_baseTrain, y_baseTrain)\n",
    "\n",
    "y_basePred = model.predict(x_baseTest)\n",
    "\n",
    "# evaluating the model\n",
    "print(\"Training Accuracy :\", model.score(x_baseTrain, y_baseTrain))\n",
    "print(\"Testing Accuracy :\", model.score(x_baseTest, y_baseTest))\n",
    "print(\"ROC AUC Score :\", roc_auc_score(y_baseTest, y_basePred))\n",
    "\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(y_baseTest, y_basePred)\n",
    "print(cm)\n",
    "\n",
    "# classification report\n",
    "cr = classification_report(y_baseTest, y_basePred)\n",
    "print(cr)\n",
    "\n",
    "\n",
    "# cross validation\n",
    "scoring = {'accuracy' : make_scorer(accuracy_score), \n",
    "           'precision' : make_scorer(precision_score),\n",
    "           'recall' : make_scorer(recall_score), \n",
    "           'f1_score' : make_scorer(f1_score)}\n",
    "cvs = cross_validate(estimator = model, X = x_baseTrain, y = y_baseTrain, cv = 10,scoring=scoring)\n",
    "print(cvs)\n",
    "print(\"Mean Accuracy :\", np.mean(cvs['test_accuracy']))\n",
    "print(\"Mean Standard Deviation :\", np.std(cvs['test_accuracy']))\n",
    "print(\"Mean precision score :\", np.mean(cvs['test_precision']))\n",
    "print(\"Mean Standard Deviation precision score :\", np.std(cvs['test_precision']))\n",
    "print(\"Mean recall score :\", np.mean(cvs['test_recall']))\n",
    "print(\"Mean Standard Deviation recall score :\", np.std(cvs['test_recall']))\n",
    "print(\"Mean f1 score :\", np.mean(cvs['test_f1_score']))\n",
    "print(\"Mean Standard Deviation f1 score :\", np.std(cvs['test_f1_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy : 0.9230680106592515\n",
      "Testing Accuracy : 0.8948364422816978\n",
      "ROC AUC Score : 0.7702947725880979\n",
      "[[2971  153]\n",
      " [ 236  339]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.95      0.94      3124\n",
      "           1       0.69      0.59      0.64       575\n",
      "\n",
      "    accuracy                           0.89      3699\n",
      "   macro avg       0.81      0.77      0.79      3699\n",
      "weighted avg       0.89      0.89      0.89      3699\n",
      "\n",
      "{'fit_time': array([1.05914116, 1.04225183, 1.05915332, 1.0701499 , 1.06311822,\n",
      "       1.06418085, 1.07812858, 1.03903604, 1.04836583, 1.08410001]), 'score_time': array([0.00997281, 0.01097155, 0.01097107, 0.01097035, 0.01097012,\n",
      "       0.00998569, 0.01095772, 0.01562119, 0.01093674, 0.01097178]), 'test_accuracy': array([0.90856481, 0.88194444, 0.90625   , 0.9188876 , 0.91425261,\n",
      "       0.90845886, 0.89571263, 0.90961761, 0.90603248, 0.89791183]), 'test_precision': array([0.74774775, 0.64545455, 0.74311927, 0.79439252, 0.76576577,\n",
      "       0.73684211, 0.68376068, 0.72357724, 0.75      , 0.70642202]), 'test_recall': array([0.61940299, 0.52985075, 0.60447761, 0.63909774, 0.63909774,\n",
      "       0.63157895, 0.60150376, 0.66917293, 0.58646617, 0.57894737]), 'test_f1_score': array([0.67755102, 0.58196721, 0.66666667, 0.70833333, 0.69672131,\n",
      "       0.68016194, 0.64      , 0.6953125 , 0.65822785, 0.63636364])}\n",
      "Mean Accuracy : 0.9047632891142856\n",
      "Mean Standard Deviation : 0.009972935367105156\n",
      "Mean precision score : 0.7297081891532413\n",
      "Mean Standard Deviation precision score : 0.040301152584487014\n",
      "Mean recall score : 0.6099596004937717\n",
      "Mean Standard Deviation recall score : 0.03713965158332534\n",
      "Mean f1 score : 0.6641305472783068\n",
      "Mean Standard Deviation f1 score : 0.0354553875077925\n"
     ]
    }
   ],
   "source": [
    "# MODELLING ____gradient boosting Classifier\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "\n",
    "model = GradientBoostingClassifier()\n",
    "model.fit(x_baseTrain, y_baseTrain)\n",
    "\n",
    "y_basePred = model.predict(x_baseTest)\n",
    "\n",
    "# evaluating the model\n",
    "print(\"Training Accuracy :\", model.score(x_baseTrain, y_baseTrain))\n",
    "print(\"Testing Accuracy :\", model.score(x_baseTest, y_baseTest))\n",
    "print(\"ROC AUC Score :\", roc_auc_score(y_baseTest, y_basePred))\n",
    "\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(y_baseTest, y_basePred)\n",
    "print(cm)\n",
    "\n",
    "# classification report\n",
    "cr = classification_report(y_baseTest, y_basePred)\n",
    "print(cr)\n",
    "\n",
    "\n",
    "# cross validation\n",
    "scoring = {'accuracy' : make_scorer(accuracy_score), \n",
    "           'precision' : make_scorer(precision_score),\n",
    "           'recall' : make_scorer(recall_score), \n",
    "           'f1_score' : make_scorer(f1_score)}\n",
    "cvs = cross_validate(estimator = model, X = x_baseTrain, y = y_baseTrain, cv = 10,scoring=scoring)\n",
    "print(cvs)\n",
    "print(\"Mean Accuracy :\", np.mean(cvs['test_accuracy']))\n",
    "print(\"Mean Standard Deviation :\", np.std(cvs['test_accuracy']))\n",
    "print(\"Mean precision score :\", np.mean(cvs['test_precision']))\n",
    "print(\"Mean Standard Deviation precision score :\", np.std(cvs['test_precision']))\n",
    "print(\"Mean recall score :\", np.mean(cvs['test_recall']))\n",
    "print(\"Mean Standard Deviation recall score :\", np.std(cvs['test_recall']))\n",
    "print(\"Mean f1 score :\", np.mean(cvs['test_f1_score']))\n",
    "print(\"Mean Standard Deviation f1 score :\", np.std(cvs['test_f1_score']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
